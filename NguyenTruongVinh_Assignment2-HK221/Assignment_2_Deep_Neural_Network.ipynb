{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LGpbq4tSiOw"
      },
      "source": [
        "# Bài tập về nhà 2\n",
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e8vwb3zTACs"
      },
      "source": [
        "### Tóm tắt nội dung\n",
        "Trong bài tập này, các bạn sẽ sử dụng kiến thức đã học về neural networks để giải quyết bài toán phân lớp (classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbebJwTgTC9a"
      },
      "source": [
        "## Mục lục\n",
        "* [1. Hướng Dẫn](#C1)\n",
        "    * [1.1. Cấu trúc file](#C1_1)\n",
        "    * [1.2. Sử dụng gradient_check.py ](#C1_2)\n",
        "* [2. Sử dụng util.py ](#C2)\n",
        "    * [2.1. Hàm load_npy()](#C2_1)\n",
        "    * [2.2. Hàm load_list()](#C2_2)\n",
        "    * [2.3. Hàm save_list()](#C2_3)\n",
        "    * [2.4. Hàm get_vehicle_data()](#C2_4)\n",
        "    * [2.5. Hàm read_mnist_gz()](#C2_5)\n",
        "    * [2.6. Hàm get_mnist_data()](#C2_6)\n",
        "    * [2.7. Hàm get_bat_data()](#C2_7)\n",
        "    * [2.8. Hàm visualize_point()](#C2_8)\n",
        "    * [2.9. Hàm plot_loss()](#C2_9)\n",
        "    * [2.10. Hàm normalize()](#C2_10)\n",
        "    * [2.11. Hàm create_one_hot()](#C2_11)\n",
        "    * [2.12. Hàm add_one()](#C2_12)\n",
        "* [3. Hiện thực neural network bằng numpy](#C3)\n",
        "    * [3.1. Dữ liệu bat](#C3_1)\n",
        "    * [3.2. Dữ liệu fashion MNIST](#C3_2)\n",
        "    * [3.3. Các nội dung cần hiện thực](#C3_3)\n",
        "        * [3.3.1 Hiện thực các activation functions cơ bản [TODO 1.1]](#C3_3_1)\n",
        "        * [3.3.2 Hiện thực giải thuật Feedforward và Backpropagation cho từng layer [TODO 1.2]](#C3_3_2)\n",
        "        * [3.3.3 Tính độ lỗi trong hàm compute_loss [TODO 1.3]](#C3_3_3)\n",
        "        * [3.3.4 Áp dụng regularization vào mạng NN [TODO 1.4]](#C3_3_4)\n",
        "        * [3.3.5 Hiện thực giải thuật backpropagation cho class NeuralNet [TODO 1.5]](#C3_3_5)\n",
        "        * [3.3.6 Cập nhật $w$ theo Batch Gradient Descent và Mini-batch Gradient Descent [TODO 1.6]](#C3_3_6)\n",
        "    * [3.4. Huấn luyện mô hình](#C3_4)\n",
        "        * [3.4.1 Lựa chọn thông số](#C3_4_1)\n",
        "        * [3.4.2 Bat Classification](#C3_4_2)\n",
        "        * [3.4.3 MNIST Classification](#C3_4_3)\n",
        "* [4. Sử dụng Tensorflow để huấn luyện mạng NN [TODO 1.7]](#C4)   \n",
        "* [5. Kết quả](#C5)\n",
        "    * [5.1. Bat Classification](#C5_1)\n",
        "    * [5.2. MNIST Classification](#C5_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f8tLypk1elE"
      },
      "source": [
        "## 1. Hướng dẫn <a id='C1'></a>\n",
        "Chương này sẽ hướng dẫn những điều cần thiết để hoàn thành bài tập này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kESB1ROATIF7"
      },
      "source": [
        "Để có thể hoàn tất bài tập này, các bạn cần nắm rõ những kiến thức sau: \n",
        "- Neural Networks - Fully connected networks là gì, nguyên tắc hoạt động\n",
        "ra sao.\n",
        "- Giải thuật Feedforward và BackPropagation trong bài toán NN.\n",
        "- Giải thuật gradient descent - Batch and Mini-batch.\n",
        "- Regularization để tránh overfitting trong NN.\n",
        "\n",
        "Bạn có thể tham khảo lại bài giảng của lớp để nắm vững các nội dung này. Ngoài ra, các bạn có thể đặt câu hỏi cho đội ngũ giảng dạy nếu có thắc mắc.\n",
        "\n",
        "Bài tập này sẽ gồm có hai bài chính:\n",
        "- Bài 1: phân loại dữ liệu ba lớp dùng neural networks.\n",
        "- Bài 2: phân loại tập fashion MNIST.\n",
        "\n",
        "Yêu cầu dành cho các bạn trong là giải quyết hai bài trên bằng **cả numpy và TensorFlow**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLgPOx1DpSwb"
      },
      "source": [
        "### 1.1. Cấu trúc file <a id='C1_1'></a>\n",
        "Bài tập lớn này được đi kèm với các file sau: \n",
        "- util.py: cung cấp các hàm để đọc dữ liệu trong thưc mục data thành các\n",
        "ma trận numpy. Bạn không cần chỉnh sửa file này.\n",
        "- activation_np.py: các hàm activation cơ bản sẽ cần người học thực thi\n",
        "- dnn_np.py: cung cấp các hàm dựng sẵn để giải quyết bài 1 và bài 2,\n",
        "dùng numpy.\n",
        "- dnn_tf.py: sử dụng Tensorflow để giải quyết bài 1 và 2.\n",
        "- bat.dat: dữ liệu của bài 1.\n",
        "- /fashion-mnist/*.gz: dữ liệu của bài 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Sử dụng gradient_check.py <a id='C1_2'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "File này cung cấp các hàm cho bạn kiếm tra tính toán gradient của bạn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"gradient_check.py\n",
        "This file provides functions for you to check your gradient computation\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\"\n",
        "    function to determine relative error between expected output results from our actual implementation of a layer\n",
        "    :param x: expected output, arbitrary shape\n",
        "    :param y: output from our implementation\n",
        "    :return:  relative error > 1e-2 means that the result is probably wrong\n",
        "                             <= e-7, you should be happy\n",
        "    \"\"\"\n",
        "    return np.max(np.abs(x - y)/ np.maximum(1e-8, np.abs(x) + np.abs(y)))\n",
        "\n",
        "\n",
        "def eval_numerical_gradient(layer,x, df, verbose = True, h = 0.00001):\n",
        "    \"\"\"\n",
        "    a naive implementation of numerical gradient of f at x\n",
        "    :param f: should be a function that takes a single argument x\n",
        "    :param x: is the point to evaluate the gradient at\n",
        "    :param verbose:\n",
        "    :param h:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    fw = layer.forward(x)\n",
        "    grad = np.zeros_like(layer.w)\n",
        "    it = np.nditer(layer.w, flags = ['multi_index'], op_flags = ['readwrite'])\n",
        "    while not it.finished:\n",
        "\n",
        "        ix = it.multi_index\n",
        "        oldval = layer.w[ix].copy()\n",
        "        layer.w[ix] = oldval + h\n",
        "        fxph  = layer.forward(x) #evaluate f(x+h)\n",
        "        layer.w[ix] = oldval - h\n",
        "        fxmh  = layer.forward(x)\n",
        "        layer.w[ix] = oldval.copy()\n",
        "\n",
        "        grad[ix] = np.sum((fxph - fxmh)*df)/ (2*h)\n",
        "        if verbose:\n",
        "            print(ix, grad[ix])\n",
        "        it.iternext()\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caPRYRLnVwu8"
      },
      "source": [
        "## 2. Sử dụng util.py <a id='C2'></a>\n",
        "\n",
        "Chương này mô tả các feature trong file util.py."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzajf88D1elI"
      },
      "source": [
        "Sử dụng các thư viện sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gHJ1OZONpud"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This files helps you read data from data files\n",
        "\"\"\"\n",
        "import pickle\n",
        "import gzip\n",
        "import glob\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28JBrurd1elL"
      },
      "source": [
        "### 2.1. Hàm load_npy() <a id='C2_1'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVIKDhzh1elM"
      },
      "source": [
        "Mục đích: Sử dụng để load numpy data file\n",
        "- Input: file_name\n",
        "- Output: obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JV27SB0l0fD"
      },
      "outputs": [],
      "source": [
        "def load_npy(file_name):\n",
        "    \"\"\"load_npy\n",
        "    Load numpy data file. This is needed as python 2.7 pickle uses ascii as default encoding method but python 3.x uses utf-8.abs\n",
        "\n",
        "    :param file_name: npy file path\n",
        "    \n",
        "    :return obj: loaded numpy object\n",
        "    \"\"\"\n",
        "    \n",
        "    if (sys.version_info[0] >= 3):\n",
        "        obj = np.load(file_name, encoding='latin1')\n",
        "    elif (sys.version_info[0] >=2):\n",
        "        obj = np.load(file_name)\n",
        "    \n",
        "    return obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BesGbm3E1elN"
      },
      "source": [
        "### 2.2. Hàm load_list() <a id='C2_2'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXq9HYyV1elO"
      },
      "source": [
        "Mục đích: Sử dụng để load 1 list object vào file_name\n",
        "- Input: file_name\n",
        "- Output: list_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB6CKQKm1elO"
      },
      "outputs": [],
      "source": [
        "def load_list(file_name):\n",
        "    \"\"\"load_list\n",
        "    Load a list object to file_name.\n",
        "\n",
        "    :param file_name: string, file name.\n",
        "    \"\"\"\n",
        "    end_of_file = False\n",
        "    list_obj = [] \n",
        "    f = open(file_name, 'rb')\n",
        "    python_version = sys.version_info[0]\n",
        "    while (not end_of_file):\n",
        "        try:\n",
        "            if (python_version >= 3):\n",
        "                list_obj.append(pickle.load(f, encoding='latin1'))\n",
        "            elif (python_version >=2):\n",
        "                list_obj.append(pickle.load(f))\n",
        "        except EOFError:\n",
        "            end_of_file = True\n",
        "            print(\"EOF Reached\")\n",
        "\n",
        "    f.close()\n",
        "    return list_obj "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtUXzgXa1elP"
      },
      "source": [
        "### 2.3. Hàm save_list() <a id='C2_3'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgPGQJNA1elP"
      },
      "source": [
        "Mục đích: Sử dụng để save 1 list object vào trong file_name\n",
        "- Input: list_obj, file_name\n",
        "- Output: Save data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J7H6UCb1elP"
      },
      "outputs": [],
      "source": [
        "def save_list(list_obj, file_name):\n",
        "    \"\"\"save_list\n",
        "    Save a list object to file_name\n",
        "    \n",
        "    :param list_obj: List of objects to be saved.\n",
        "    :param file_name: file name.\n",
        "    \"\"\"\n",
        "\n",
        "    f = open(file_name, 'wb')\n",
        "    for obj in list_obj:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "    f.close() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oviD4R61elQ"
      },
      "source": [
        "### 2.4. Hàm get_vehicle_data() <a id='C2_4'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrEaD6I1elQ"
      },
      "source": [
        "Mục đích: Sử dụng để load vehicle data trong thư mục\n",
        "- Input: Lấy data trong thư mục\n",
        "- Output: List của data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frvdaf0y1elQ"
      },
      "outputs": [],
      "source": [
        "def get_vehicle_data():\n",
        "    \"\"\"\n",
        "    Load vehicle data and return it as a list: [train_x, train_y, test_x, test_y]\n",
        "    \"\"\"\n",
        "    print('Reading vehicle data...')\n",
        "    train_x, train_y, test_x, test_y = load_list('./data/vehicles.dat')\n",
        "    train_x = np.transpose(train_x, (2,0,1))\n",
        "    test_x = np.transpose(test_x, (2,0,1)) \n",
        "\n",
        "    print('Done reading')\n",
        "    return train_x, train_y, test_x, test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "numqxZoz1elR"
      },
      "source": [
        "### 2.5. Hàm read_mnist_gz() <a id='C2_5'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNLA1gUG1elR"
      },
      "source": [
        "Mục đích: Dùng để đọc fashion MNIST data dưới dạng nén\n",
        "- Input: data_path, offset\n",
        "- Output: dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qphe51lS1elR"
      },
      "outputs": [],
      "source": [
        "def read_mnist_gz(data_path, offset):\n",
        "    with gzip.open(data_path, 'rb') as f:\n",
        "        dataset = np.frombuffer(f.read(), dtype=np.uint8, offset=offset)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tt50Vtc1elS"
      },
      "source": [
        "### 2.6. Hàm get_mnist_data() <a id='C2_6'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmCuNsbw1elS"
      },
      "source": [
        "Mục đích: Dùng để load fashion MNIST data\n",
        "- Input: sampling_step\n",
        "- Output: List của data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r08oC5sO1elS",
        "outputId": "07b2d610-fd47-4eb8-cf6b-484ee0ce0f50"
      },
      "outputs": [],
      "source": [
        "def get_mnist_data(sampling_step=20):\n",
        "    print('Reading fashion MNIST data...')\n",
        "    train_x = read_mnist_gz('./data/fashion-mnist/train-images-idx3-ubyte.gz', 16)\n",
        "    train_y = read_mnist_gz('./data/fashion-mnist/train-labels-idx1-ubyte.gz', 8)\n",
        "    test_x = read_mnist_gz('./data/fashion-mnist/t10k-images-idx3-ubyte.gz', 16)\n",
        "    test_y = read_mnist_gz('./data/fashion-mnist/t10k-labels-idx1-ubyte.gz', 8)\n",
        "    num_train = len(train_y)\n",
        "    num_test = len(test_y)\n",
        "\n",
        "    train_x = train_x.reshape((num_train, 28*28))\n",
        "    test_x = test_x.reshape((num_test, 28*28))\n",
        "\n",
        "    val_x = train_x[50000:,:]\n",
        "    val_y = train_y[50000:]\n",
        "    train_x = train_x[:50000,:]\n",
        "    train_y = train_y[:50000]\n",
        "\n",
        "    train_x = train_x[0::sampling_step,:]\n",
        "    train_y = train_y[0::sampling_step]\n",
        "    val_x = val_x[0::sampling_step,:]\n",
        "    val_y = val_y[0::sampling_step]\n",
        "    test_x = test_x[0::sampling_step,:]\n",
        "    test_y = test_y[0::sampling_step]\n",
        " \n",
        "    print(\"Done reading\")\n",
        "    return train_x.astype(np.float32), train_y, val_x.astype(np.float32), val_y, test_x.astype(np.float32), test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.7. Hàm get_bat_data() <a id='C2_7'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mục đích: Đọc liệu dơi\n",
        "- Input: bat data\n",
        "- Output: train_x, train_y, test_x, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bat_data():\n",
        "    \"\"\"\n",
        "    Load bat data and return it as a list: [train_x, train_y, test_x, test_y]\n",
        "    \"\"\"\n",
        "    print('Reading bat data...')\n",
        "    train_x, train_y, test_x, test_y = load_list('./data/bat.dat')\n",
        "\n",
        "    print('Done reading')\n",
        "    return train_x, train_y, test_x, test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.8. Hàm visualize_point() <a id='C2_8'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mục đích: Hiện thực hóa quá trình huấn luyện"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_point(x, y, y_hat, fig=1):\n",
        "    \"\"\"visualize_point\n",
        "    \n",
        "    This funciton scatter data points (in x) and color them according to y and y_hat for comparison\n",
        "    Both figures should be similar\n",
        "    :param x: data points, each point has two dimensions (x1, x2)\n",
        "    :param y: actual labels of the data points\n",
        "    :param y_hat: predicted labels of the data points\n",
        "    \"\"\"\n",
        "    \n",
        "    color_map = np.asarray([\n",
        "            [0,0,0],\n",
        "            [1,1,0],\n",
        "            [0,0,1]\n",
        "            ])\n",
        "    \n",
        "    fig = plt.figure(fig, figsize = (12,6))\n",
        "    plt.clf()\n",
        "   \n",
        "    if (y.ndim == 2):\n",
        "        if (y.shape[1] > 1 and np.unique(y) == 2):\n",
        "            # One-hot, probably\n",
        "            color_list = color_map[np.argmax(y,1), :]\n",
        "        else:\n",
        "            # int label\n",
        "            color_list = color_map[y.flatten(), :]\n",
        "    elif (y.ndim == 1):\n",
        "        color_list = color_map[y,:]\n",
        "    else:\n",
        "        raise ValueError(\"y should be of shape (batch_size, ) or (batch_size, num_class)\")\n",
        "\n",
        "    color_list = color_map[y,:]\n",
        "    ax = plt.subplot(1,2,1)\n",
        "    ax.set_title(\"Actual classes\")\n",
        "    ax.scatter(x[:,0], x[:,1], color = color_list)\n",
        "    plt.axis('equal')\n",
        "\n",
        "    c = np.copy(y_hat)\n",
        "    c = np.argmax(c, 1)\n",
        "    color_list = color_map[c,:]\n",
        "    ax = plt.subplot(1,2,2)\n",
        "    ax.set_title(\"Prediction\")\n",
        "    ax.scatter(x[:,0], x[:,1], color = color_list)\n",
        "\n",
        "    plt.axis('equal')\n",
        "    plt.ion() \n",
        "    plt.draw()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.9. Hàm plot_loss() <a id='C2_9'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mục đích: Hiện thực đồ thị"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss(loss, fig=1, color='b'):\n",
        "    plt.figure(fig)\n",
        "    plt.clf()\n",
        "    plt.plot(loss, color='b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.10. Hàm normalize() <a id='C2_10'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mục đích: Chuẩn hóa\n",
        "- Input: train_x, val_x, test_x\n",
        "- Output: train_x, val_x, test_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(train_x, val_x, test_x):\n",
        "    \"\"\"normalize\n",
        "    This function computes train mean and standard deviation on all pixels then applying data scaling on train_x, val_x and test_x using these computed values\n",
        "\n",
        "    :param train_x: train samples, shape=(num_train, num_feature)\n",
        "    :param val_x: validation samples, shape=(num_val, num_feature)\n",
        "    :param test_x: test samples, shape=(num_test, num_feature)\n",
        "    \"\"\"\n",
        "    # train_mean and train_std should have the shape of (1, 1)\n",
        "    train_mean = np.mean(train_x, axis=(0,1), dtype=np.float64, keepdims=True)\n",
        "    train_std = np.std(train_x, axis=(0,1), dtype=np.float64, keepdims=True)\n",
        "\n",
        "    train_x = (train_x-train_mean)/train_std\n",
        "    val_x = (val_x-train_mean)/train_std\n",
        "    test_x = (test_x-train_mean)/train_std\n",
        "    return train_x, val_x, test_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.11. Hàm create_one_hot() <a id='C2_11'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mục đích: Tạo ma trận one-hot \n",
        "- Input: labels, num_k\n",
        "- Output: eye_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_one_hot(labels, num_k=10):\n",
        "    \"\"\"create_one_hot\n",
        "    This function creates a one-hot (one-of-k) matrix based on the given labels\n",
        "\n",
        "    :param labels: list of labels, each label is one of 0, 1, 2,... , num_k - 1\n",
        "    :param num_k: number of classes we want to classify\n",
        "    \"\"\"\n",
        "    eye_mat = np.eye(num_k)\n",
        "    return eye_mat[labels, :].astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.12. Hàm add_one() <a id='C2_12'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Input: x\n",
        "- Output: x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_one(x):\n",
        "    \"\"\"add_one\n",
        "    \n",
        "    This function add ones as an additional feature for x\n",
        "    :param x: input data\n",
        "    \"\"\"\n",
        "    x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n",
        "    return x\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    get_mnist_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yzZpItjWqe4"
      },
      "source": [
        "## 3. Hiện thực neural network bằng numpy <a id='C3'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecv42PtbW2YX"
      },
      "source": [
        "### 3.1. Dữ liệu bat <a id='C3_1'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cPITStuW6wW"
      },
      "source": [
        "Tập dữ liệu bat là tập gồm có 3 lớp, được gán nhãn lớp 0, 1 và 2. Ta có thể đọc tập dữ liệu này bằng hàm `get_bat_data()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "qwTC-jzcWiFS",
        "outputId": "f789bbf5-a86c-488f-9119-c430ad6f8b80"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, test_x, test_y = get_bat_data()\n",
        "visualize_point(train_x, train_y, train_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMUaX4rn1elY"
      },
      "source": [
        "### 3.2. Dữ liệu fashion MNIST <a id='C3_2'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dữ liệu này giống với dữ liệu trong bài tập softmax regression. Bạn có thể sử dụng hàm get data tương ứng để lấy dữ liệu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3. Các nội dung cần hiện thực <a id='C3_3'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rul5BLAlOMOC"
      },
      "source": [
        "### 3.3.1 Hiện thực các activation functions cơ bản [TODO 1.1] <a id='C3_3_1'></a>\n",
        "Hiện thực các hàm activation cơ bản trong mạng Neural Networks tại file `activation_np.py`. Trong file có sẵn hàm `sigmoid`, `relu`, `tanh` và `softmax`, ta nên thực hiện code trong hàm này. Ngoài ra ta sẽ hiện thực các hàm cơ bản tính gradient của các hàm trên với input của các hàm - `sigmoid_grad`, `reLU_grad`,\n",
        "và `tanh_grad`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzRsJ5bWoGt9"
      },
      "source": [
        "#### TODO 1.1: sigmoid "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB53DTTQmqCA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    \"\"\"sigmoid\n",
        "    TODO: \n",
        "    Sigmoid function. Output = 1 / (1 + exp(-x)).\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "    #[TODO 1.1]\n",
        "    return 1/(1+np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO 1.1: sigmoid_grad "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid_grad(a):\n",
        "    \"\"\"sigmoid_grad\n",
        "    TODO:\n",
        "    Compute gradient of sigmoid with respect to input. g'(x) = g(x)*(1-g(x))\n",
        "    :param a: output of the sigmoid function\n",
        "    \"\"\"\n",
        "    #[TODO 1.1]\n",
        "    return (a)*(1-a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSU-sZg3oM7N"
      },
      "source": [
        "#### TODO 1.1: reLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yguGEWe1msRo"
      },
      "outputs": [],
      "source": [
        "def reLU(x):\n",
        "    \"\"\"reLU\n",
        "    TODO:\n",
        "    Rectified linear unit function. Output = max(0,x).\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "    #[TODO 1.1]\n",
        "    return np.maximum(0,x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO 1.1: reLU_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reLU_grad(a):\n",
        "    \"\"\"reLU_grad\n",
        "    TODO:\n",
        "    Compute gradient of ReLU with respect to input\n",
        "    :param x: output of ReLU\n",
        "    \"\"\"\n",
        "    #[TODO 1.1]\n",
        "    grad = np.copy(a)\n",
        "    grad[grad <= 0] = 0\n",
        "    grad[grad > 0] = 1\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO 1.1: tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    \"\"\"tanh\n",
        "    TODO:\n",
        "    Tanh function.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "    #[TODO 1.1]\n",
        "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO 1.1: tanh_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tanh_grad(a):\n",
        "    \"\"\"tanh_grad\n",
        "    TODO:\n",
        "    Compute gradient for tanh w.r.t input\n",
        "    :param a: output of tanh\n",
        "    \"\"\"\n",
        "    #[TODO 1.1]\n",
        "    return 1 - a**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Các hàm softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    \"\"\"softmax\n",
        "    TODO:\n",
        "    Softmax function.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "\n",
        "    exp_scores = np.exp(x)\n",
        "    probs = exp_scores/np.sum(exp_scores, axis = 1, keepdims = True)\n",
        "    return probs\n",
        "\n",
        "\n",
        "def softmax_minus_max(x):\n",
        "    \"\"\"softmax_minus_max\n",
        "    TODO:\n",
        "    Stable softmax function.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "\n",
        "    exp_scores = np.exp(x - np.max(x, axis = 1, keepdims = True))\n",
        "    probs = exp_scores/np.sum(exp_scores, axis = 1, keepdims = True)\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-nakFutobwo"
      },
      "source": [
        "### 3.3.2 Hiện thực giải thuật Feedforward và Backpropagation cho từng layer [TODO 1.2] <a id='C3_3_2'></a>\n",
        "\n",
        "#### Một số Ký hiệu\n",
        "\n",
        "- $L$: số layers trong mạng neural network. \n",
        "- $l = 0,1,..,L$ với $0$ là layer input và $L$ layer output.\n",
        "- $n^{[l]}$ là số neurons tại layer $l$\n",
        "- $l-1$: layer trước theo chiều forward của $l$.\n",
        "- $l+1$: layer trước theo chiều backward của $l$.\n",
        "- $\\sigma'(x)$: đạo hàm hàm activation theo x (general case cho cả đạo hàm sigmoid, tanh, relu).\n",
        "- $Z^{[l]}$: linear function values tại layer $l$.\n",
        "- $A^{[l]}$: activation function values tại layer $l$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Hàm `forward`:\n",
        "- Hàm nhận vào tham số input $X$ (là output của hidden layer trước theo chiều forward, layer $l-1$).\n",
        "- Tính linear transformation của $X$ ($A^{[l-1]}$): $Z^{[l]} = XW$.\n",
        "- Sau đó tính nonlinear transformation: $A^{[l]} = \\sigma(Z^{[l]})$ với $\\sigma$ là hàm activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Hàm `backward`:\n",
        "\n",
        "- Hàm nhận vào 2 tham số input `X`(là output của hidden layer trước đó theo chiều forward) với `delta_prev` (delta trước đó theo chiều backward).\n",
        "- Tính delta tại layer $l$: \n",
        "    \n",
        "    $$\\delta^{[l]} = \\frac{\\partial J}{\\partial A^{[l]}}\\frac{\\partial A^{[l]}}{\\partial Z^{[l]}} =  \\delta^{[l+1]} * \\sigma'(Z^{[l]})$$ \n",
        "\n",
        "    Chú ý: $*$ operation là element-wise multiplication.\n",
        "- Tính W_grad (without regularization): $\\nabla W^{[l]} = (A^{[l-1]})^T\\delta^{[l]} $\n",
        "- With regularization:  $\\nabla W^{[l]} = (A^{[l-1]})^T\\delta^{[l]} + \\frac{\\lambda}{N} W^{[l]}$ với $\\lambda$ là hệ số regularization (hyperparameter mình sẽ chọn)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZP6cBp5osT5"
      },
      "source": [
        "#### TODO 1.2: Feedforward và Backpropagation\n",
        "Tham khảo hàm `forward` và `backward` trong class `Layer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWQ-uSJdtMzK"
      },
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "    def __init__(self, w_shape, activation, reg = 1e-5):\n",
        "        \"\"\"__init__\n",
        "\n",
        "        :param w_shape: create w with shape w_shape using normal distribution\n",
        "        :param activation: string, indicating which activation function to be used\n",
        "        \"\"\"\n",
        "        \n",
        "        mean = 0\n",
        "        std = 1\n",
        "        self.w = np.random.normal(0, np.sqrt(2./np.sum(w_shape)), w_shape)\n",
        "        self.activation = activation\n",
        "        self.reg = reg\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward\n",
        "        This function compute the output of this layer\n",
        "        \n",
        "        :param x: input\n",
        "        \"\"\"\n",
        "        # [TODO 1.2]\n",
        "        result = np.dot(x, self.w)\n",
        "        \n",
        "        # Compute different types of activation\n",
        "        if (self.activation == 'sigmoid'):\n",
        "            result = sigmoid(result)\n",
        "        elif (self.activation == 'relu'):\n",
        "            result = reLU(result)\n",
        "        elif (self.activation == 'tanh'):\n",
        "            result = tanh(result)\n",
        "        elif (self.activation == 'softmax'):\n",
        "            result = softmax_minus_max(result)\n",
        "\n",
        "        self.output = result\n",
        "        return result\n",
        "\n",
        "    def backward(self, x, delta_dot_w_prev):\n",
        "        \"\"\"backward\n",
        "        This function compute the gradient of the loss function with respect to the parameter (w) of this layer\n",
        "\n",
        "        :param x: input of the layer\n",
        "        :param delta_dot_w_prev: delta^(l+1) dot product with w^(l+1)T, computed from the next layer (in feedforward direction) or previous layer (in backpropagation direction)\n",
        "        \"\"\"\n",
        "        # [TODO 1.2]\n",
        "        # Compute delta_last factor from the output\n",
        "        if(self.activation == 'sigmoid'):\n",
        "            g = self.output.copy() \n",
        "            delta = delta_dot_w_prev*sigmoid_grad(g)\n",
        "            w_grad = (x.T).dot(delta)\n",
        "        \n",
        "        elif(self.activation == 'tanh'):\n",
        "            g = self.output.copy() \n",
        "            delta = delta_dot_w_prev*tanh_grad(g)\n",
        "            w_grad = (x.T).dot(delta)\n",
        "\n",
        "        elif(self.activation == 'relu'):\n",
        "            #backprop ReLU nonlinearity here\n",
        "            g = self.output\n",
        "            delta = delta_dot_w_prev.copy()\n",
        "            delta[g <= 0] = 0\n",
        "            w_grad =  np.dot( x.T, delta)\n",
        "\n",
        "        # [TODO 1.4] Implement L2 regularization on weights here\n",
        "        w_grad +=  self.reg*self.w\n",
        "        return w_grad, delta.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRCYuQCfpl1T"
      },
      "source": [
        "### 3.3.3 Tính độ lỗi trong hàm compute_loss [TODO 1.3] <a id='C3_3_3'></a>\n",
        "- Cross-entropy, không regularization: $$J = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C y_{ik}\\log(a_{ik}^{[L]})$$\n",
        "- Cross-entropy, L2 regularization: $$J = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C y_{ik}\\log(a_{ik}^{[L]}) + \\frac{\\lambda}{2N}\\sum_{l=1}^L\\|\\mathbf{W}^{[l]} \\|_2^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwSNZlg2qFCv"
      },
      "source": [
        "#### TODO 1.3: compute_loss\n",
        "Tham khảo hàm `compute_loss` trong class `NeuralNet`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPll6bewuoqH"
      },
      "source": [
        "### 3.3.4 Áp dụng regularization vào mạng NN [TODO 1.4] <a id='C3_3_4'></a>\n",
        "Việc thay đổi hàm loss cũng sẽ dẫn đến sự thay đổi giá trị gradient của hàm loss với tham số từng hàm. Quá trình update $w$ của từng layers sẽ có thêm\n",
        "thành phần regularization như sau:\n",
        "\n",
        "$$\\nabla \\mathbf{W}^{[l]} = (\\mathbf{A}^{[l-1]})^T \\delta^{[l]} + \\frac{\\lambda}{N} W^{[l]}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BODXyv9Hu8gm"
      },
      "source": [
        "#### TODO 1.4: w_grad\n",
        "Tham khảo hàm `backward` trong class `Layer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhgYv_vEyNI2"
      },
      "source": [
        "### 3.3.5 Hiện thực giải thuật backpropagation cho class NeuralNet [TODO 1.5]<a id='C3_3_5'></a>\n",
        "\n",
        "Sau khi đã thực thi forward và backward cho từng layer tại class Layer. Mục tiêu của người học là ghép các layer đó vào trong class NeuralNet nhầm thực hiện\n",
        "đầy đủ giải thuật backpropagation cho quá trình chạy cho toàn mạng. Quá trình forward cho mạng NN đã được thực thi tại class NeuralNet với hàm forward. Nhiệm vụ của người học là thực hiện hàm backward của class NeuralNet để tính gradient của loss tương ứng với weight của từng layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fobSVpxdzJCK"
      },
      "source": [
        "#### TODO 1.5: backward\n",
        "Tham khảo hàm `backward` trong class `NeuralNet`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class NeuralNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNet(object):\n",
        "    def __init__(self, num_class=2, reg = 1e-5):\n",
        "        self.layers = []\n",
        "        self.momentum = []\n",
        "        self.reg = reg\n",
        "        self.num_class = num_class\n",
        "        \n",
        "    def add_linear_layer(self, w_shape, activation):\n",
        "        \"\"\"add_linear_layer\n",
        "\n",
        "        :param w_shape: create w with shape w_shape using normal distribution\n",
        "        :param activation: string, indicating which activation function to be used\n",
        "        \"\"\"\n",
        "        if(len(self.layers) != 0):\n",
        "            if(w_shape[0] != self.layers[-1].w.shape[-1]):\n",
        "                raise ValueError(\"Shape does not match between the added layer and previous hidden layer.\")\n",
        "\n",
        "        if(activation == 'sigmoid'):\n",
        "            self.layers.append(Layer(w_shape, 'sigmoid', self.reg))\n",
        "        elif(activation == 'relu'):\n",
        "            self.layers.append(Layer(w_shape, 'relu', self.reg)) \n",
        "        elif(activation == 'tanh'):\n",
        "            self.layers.append(Layer(w_shape, 'tanh', self.reg))\n",
        "        elif(activation == 'softmax'):\n",
        "            self.layers.append(Layer(w_shape, 'softmax', self.reg))\n",
        "        self.momentum.append(np.zeros_like(self.layers[-1].w))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward\n",
        "\n",
        "        :param x: input\n",
        "        \"\"\"\n",
        "        all_x = [x]\n",
        "        for layer in self.layers:\n",
        "            all_x.append(layer.forward(all_x[-1]))\n",
        "        \n",
        "        return all_x\n",
        "\n",
        "\n",
        "    def compute_loss(self, y, y_hat):\n",
        "        \"\"\"compute_loss\n",
        "        Compute the average cross entropy loss using y (label) and y_hat (predicted class)\n",
        "\n",
        "        :param y:  the label, the actual class of the samples. e.g. 3-class classification with 9 data samples y = [0 0 0 1 1 1 2 2 2]\n",
        "        :param y_hat: the propabilities that the given samples belong to class 1\n",
        "        \"\"\"\n",
        "\n",
        "        # [TODO 1.3]\n",
        "        # Estimating cross entropy loss from y_hat and y \n",
        "        correct_log_probs = np.log(y_hat)*y\n",
        "        data_loss = -np.sum(correct_log_probs)/y.shape[0]\n",
        "\n",
        "        #estimating regularization loss from all layers\n",
        "        reg_loss = 0.0\n",
        "        for i in range(len(self.layers)):\n",
        "            reg_loss += 0.5*self.reg*np.sum(self.layers[i].w * self.layers[i].w)\n",
        "\n",
        "        data_loss += reg_loss\n",
        "\n",
        "        return data_loss\n",
        "    \n",
        "    def backward(self, y, all_x):\n",
        "        \"\"\"backward\n",
        "\n",
        "        :param y: the label, the actual class of the samples. e.g. 3-class classification with 9 data samples y = [0 0 0 1 1 1 2 2 2]\n",
        "        :param all_x: input data and activation from every layer\n",
        "        \"\"\"\n",
        "        \n",
        "        # [TODO 1.5] Compute delta factor from the output\n",
        "        # delta = all_x[-1].copy()\n",
        "        # delta[:, y.astype(int)] -= 1\n",
        "        # delta /= y.shape[0]\n",
        "        delta = (all_x[-1].copy() - y) / y.shape[0]\n",
        "        \n",
        "        # [TODO 1.5] Compute gradient of the loss function with respect to w of softmax layer, use delta from the output\n",
        "        grad_last = self.layers[-2].output.T.dot(delta) + self.reg*self.layers[-1].w\n",
        "        # grad_last = np.dot(all_x[-2].T, delta) + (self.layers[-1].w * self.reg / y.shape[0])\n",
        "\n",
        "        grad_list = []\n",
        "        grad_list.append(grad_last)\n",
        "        \n",
        "        for i in range(len(self.layers) - 1)[::-1]:\n",
        "            prev_layer = self.layers[i+1]\n",
        "            layer = self.layers[i]\n",
        "            x = all_x[i]\n",
        "\t    # [TODO 1.5] Compute delta_dot_w_prev factor for previous layer (in backpropagation direction)\n",
        "\t    # delta_prev: delta^(l+1), in the start of this loop, delta_prev is also delta^(L) or delta_last\n",
        "\t    # delta_dot_w_prev: delta^(l+1) dot product with w^(l+1)T\n",
        "            delta_dot_w_prev = delta.dot(prev_layer.w.T)\n",
        "\t    # Use delta_dot_w_prev to compute delta factor for the next layer (in backpropagation direction)\n",
        "            grad_w, delta = layer.backward(x, delta_dot_w_prev)\n",
        "            grad_list.append(grad_w.copy())\n",
        "\n",
        "        grad_list = grad_list[::-1]\n",
        "        return grad_list\n",
        "    \n",
        "    def update_weight(self, grad_list, learning_rate):\n",
        "        \"\"\"update_weight\n",
        "        Update w using the computed gradient\n",
        "\n",
        "        :param grad: gradient computed from the loss\n",
        "        :param learning_rate: float, learning rate\n",
        "        \"\"\"\n",
        "        for i in range(len(self.layers)):\n",
        "            layer = self.layers[i]\n",
        "            grad = grad_list[i]\n",
        "            layer.w = layer.w - learning_rate * grad\n",
        "    \n",
        "    \n",
        "    def update_weight_momentum(self, grad_list, learning_rate, momentum_rate):\n",
        "        \"\"\"update_weight_momentum\n",
        "        Update w using SGD with momentum\n",
        "\n",
        "        :param grad: gradient computed from the loss\n",
        "        :param learning_rate: float, learning rate\n",
        "        :param momentum_rate: float, momentum rate\n",
        "        \"\"\"\n",
        "        for i in range(len(self.layers)):\n",
        "            layer = self.layers[i]\n",
        "            self.momentum[i] = self.momentum[i]*momentum_rate + learning_rate*grad_list[i]\n",
        "            layer.w = layer.w - self.momentum[i]\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        y_hat = self.forward(x_test)[-1]\n",
        "        return np.argmax(y_hat, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akV674wBzsi6"
      },
      "source": [
        "### 3.3.6  Cập nhật $w$ theo Batch Gradient Descent và Mini-batch Gradient Descent [TODO 1.6] <a id='C3_3_6'></a>\n",
        "Một số định nghĩa thường gặp khi huấn luyện bằng mini-batch:\n",
        "- Batch: một tập các mẫu huấn luyện.\n",
        "- Batch size: số lượng mẫu được dùng để tính feedforward, backpropagation\n",
        "và cập nhật mạng trong mỗi vòng lặp.\n",
        "- Iteration: một lần feedforward, backpropagation và cập nhật trọng số w\n",
        "trên một batch.\n",
        "- Epoch: vòng lặp lớn. Mỗi epoch bao gồm các iteration nhỏ, mỗi iteration\n",
        "sẽ cho NN học qua các batch khác nhau. Khi tất cả các iteration đã phủ\n",
        "hết toàn bộ tất cả các mẫu trong tập huấn luyện m thì coi như kết thúc\n",
        "một epoch.\n",
        "\n",
        "Ví dụ trong data bat có tất cả m = 12000:\n",
        "\n",
        "- Nếu chọn batch_size=12000, thì một epoch bao gồm 1 iteration duy\n",
        "nhất.\n",
        "- Nếu chọn batch_size=120, thì một epoch bao gồm 12000/120 = 100\n",
        "iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpjrfpjT0W85"
      },
      "source": [
        "#### TODO 1.6: minibatch_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def minibatch_train(net, train_x, train_y, cfg):\n",
        "    \"\"\"minibatch_train\n",
        "    Train your neural network using minibatch strategy\n",
        "\n",
        "    :param net: NeuralNet object\n",
        "    :param train_x: numpy tensor, train data\n",
        "    :param train_y: numpy tensor, train label\n",
        "    :param cfg: Config object\n",
        "    \"\"\"\n",
        "    # [TODO 1.6] Implement mini-batch training\n",
        "    # convert to (N,1) shape to concatenate with train_x data\n",
        "    train_y_reshape = train_y.reshape(train_y.shape[0],1)\n",
        "    \n",
        "    #Mini-batch gradient descent implementation\n",
        "    all_data_set = np.concatenate((train_x, train_y_reshape), axis = 1)\n",
        "    \n",
        "    all_loss = []\n",
        "    for e in range(cfg.num_epoch):\n",
        "        all_data_set_shuffle = np.random.shuffle(all_data_set) \n",
        "        mini_batch_data_set = np.array_split(all_data_set, cfg.batch_size, axis = 0)\n",
        "        total_loss = 0.0\n",
        "        \n",
        "        for idx, batch in enumerate(mini_batch_data_set):\n",
        "            train_batch_y = batch[:, -1].copy()\n",
        "            train_batch_y = create_one_hot(train_batch_y.astype(int), net.num_class)\n",
        "\n",
        "            train_batch_x = batch[:, :-1].copy()\n",
        "             \n",
        "            all_x = net.forward(train_batch_x)\n",
        "            y_hat = all_x[-1]\n",
        "            loss = net.compute_loss(train_batch_y, y_hat)\n",
        "            if np.isnan(loss):\n",
        "                raise ValueError(\"Loss is NaN\")\n",
        "            grads = net.backward(train_batch_y, all_x)\n",
        "            #net.update_weight(grads, cfg.learning_rate)\n",
        "            net.update_weight_momentum(grads, cfg.learning_rate, cfg.momentum_rate)\n",
        "            total_loss += loss\n",
        "\n",
        "        #printing & visualizing\n",
        "        if (cfg.visualize and e % cfg.epochs_to_draw == cfg.epochs_to_draw-1):\n",
        "            y_hat = net.forward(train_x[0::3])[-1]\n",
        "            visualize_point(train_x[0::3], train_y[0::3], y_hat)\n",
        "            plot_loss(all_loss, 2)\n",
        "            plt.show()\n",
        "            plt.pause(0.01)\n",
        "\n",
        "        print(\"Epoch %d: loss is %.5f\" % (e+1, total_loss/cfg.batch_size))\n",
        "        all_loss.append(total_loss/cfg.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e3JORB8PEfL"
      },
      "source": [
        "### 3.4. Huấn luyện mô hình <a id='C3_4'></a>\n",
        "\n",
        "Code load data và khởi tạo neural network cho data bat được đặt trong hàm bat_classification. Tương tự, code cho data mnist được đặt trong mnist_classification.\n",
        "\n",
        "Để điều chỉnh quá trình huấn luyện, bạn có thể thay đổi các thuộc tính của\n",
        "biến cfg:\n",
        "\n",
        "- num_epoch: số lượng vòng lặp cho quá trình huấn luyện.\n",
        "- batch_size: số lượng mẫu trong một batch.\n",
        "- learning_rate: hệ số học.\n",
        "- momentum_rate: hệ số động lực γ.\n",
        "- num_train: số lượng mẫu sẽ dùng để train trong train_x.\n",
        "- reg: hệ số λ cho regularization.\n",
        "- visualize: True hoặc False, xác định xem có vẽ đồ thị lúc huấn luyện hay không.\n",
        "- epochs_to_draw: số epoch cần trải qua để vẽ đồ thị."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4.1 Lựa chọn thông số <a id='C3_4_1'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- num_epoch: 1000 (cho bat classification) và 300 (cho MNIST classification)\n",
        "- batch_size: 100 (cho bat classification) và 200 (cho MNIST classification)\n",
        "- learning_rate: 0.001\n",
        "- momentum_rate: 0.9\n",
        "- num_train: train_x.shape\n",
        "- reg: 0.00015\n",
        "- visualize: True (cho bat classification), False (cho MNIST classification)\n",
        "- epochs_to_draw: 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEoEEijecqXi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from util import *\n",
        "from activation_np import *\n",
        "from gradient_check import *\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pdb\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self, num_epoch=1000, batch_size=100, learning_rate=0.0005, momentum_rate=0.9, epochs_to_draw=10, reg=0.00015, num_train=1000, visualize=True):\n",
        "        self.num_epoch = num_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum_rate = momentum_rate\n",
        "        self.epochs_to_draw = epochs_to_draw\n",
        "        self.reg = reg\n",
        "        self.num_train = num_train\n",
        "        self.visualize = visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(y_hat, test_y):\n",
        "    \"\"\"test\n",
        "    Compute the confusion matrix based on labels and predicted values \n",
        "\n",
        "    :param y_hat: predicted probabilites, output of classifier.feed_forward\n",
        "    :param test_y: test labels\n",
        "    \"\"\"\n",
        "    if (y_hat.ndim == 2):\n",
        "        y_hat = np.argmax(y_hat, axis=1)\n",
        "    num_class = np.unique(test_y).size\n",
        "    confusion_mat = np.zeros((num_class, num_class))\n",
        "\n",
        "    for i in range(num_class):\n",
        "        class_i_idx = test_y == i\n",
        "        num_class_i = np.sum(class_i_idx)\n",
        "        y_hat_i = y_hat[class_i_idx]\n",
        "        for j in range(num_class):\n",
        "            confusion_mat[i,j] = 1.0*np.sum(y_hat_i == j)/num_class_i\n",
        "\n",
        "    np.set_printoptions(precision=2)\n",
        "    print('Confusion matrix:')\n",
        "    print(confusion_mat)\n",
        "    print('Diagonal values:')\n",
        "    print(confusion_mat.flatten()[0::(num_class+1)])\n",
        "\n",
        "\n",
        "def unit_test_layer(your_layer):\n",
        "    \"\"\"unit test layer\n",
        "\n",
        "    This function is used to test layer backward and forward for a random datapoint\n",
        "    error < 1e-8 - you should be happy\n",
        "    error > e-3  - probably wrong in your implementation\n",
        "    \"\"\"\n",
        "    # generate a random data point\n",
        "    x_test = np.random.randn(1, your_layer.w.shape[0])\n",
        "    layer_sigmoid = Layer(your_layer.w.shape, your_layer.activation, reg = 0.0)\n",
        "\n",
        "    #randomize the partial derivative of the cost function w.r.t the next layer    \n",
        "    delta_prev = np.ones((1,your_layer.w.shape[1]))\n",
        "    \n",
        "    # evaluate the numerical gradient of the layer\n",
        "    numerical_grad = eval_numerical_gradient(layer_sigmoid, x_test, delta_prev, False)\n",
        "\n",
        "    #evaluate the gradient using back propagation algorithm\n",
        "    layer_sigmoid.forward(x_test)\n",
        "    w_grad, delta = layer_sigmoid.backward(x_test, delta_prev)\n",
        "\n",
        "    #print out the relative error\n",
        "    error = rel_error(w_grad, numerical_grad)\n",
        "    print(\"Relative error between numerical grad and function grad is: %e\" %error)\n",
        "\n",
        "def batch_train(net, train_x, train_y, cfg):\n",
        "    \"\"\"batch_train\n",
        "    Train the neural network using batch SGD\n",
        "\n",
        "    :param net: NeuralNet object\n",
        "    :param train_x: numpy tensor, train data\n",
        "    :param train_y: numpy tensor, train label\n",
        "    :param cfg: Config object\n",
        "    \"\"\"\n",
        "\n",
        "    train_set_x = train_x[:cfg.num_train].copy()\n",
        "    train_set_y = train_y[:cfg.num_train].copy()\n",
        "    train_set_y = create_one_hot(train_set_y, net.num_class)\n",
        "    all_loss = []\n",
        "\n",
        "    for e in range(cfg.num_epoch):\n",
        "        all_x = net.forward(train_set_x)\n",
        "        y_hat = all_x[-1]\n",
        "        loss = net.compute_loss(train_set_y, y_hat)\n",
        "        grads = net.backward(train_set_y, all_x)\n",
        "        net.update_weight(grads, cfg.learning_rate)\n",
        "\n",
        "        all_loss.append(loss)\n",
        "\n",
        "        if (e % cfg.epochs_to_draw == cfg.epochs_to_draw-1):\n",
        "            if (cfg.visualize):\n",
        "                y_hat = net.forward(train_x[0::3])[-1]\n",
        "                visualize_point(train_x[0::3], train_y[0::3], y_hat)\n",
        "            plot_loss(all_loss, 2)\n",
        "            plt.show()\n",
        "            plt.pause(0.01)\n",
        "\n",
        "        print(\"Epoch %d: loss is %.5f\" % (e+1, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqz8CAHLQdYY"
      },
      "source": [
        "### 3.4.2 Bat Classification <a id='C3_4_2'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bat_classification():\n",
        "    # Load data from file\n",
        "    # Make sure that bat.dat is in data/\n",
        "    train_x, train_y, test_x, test_y = get_bat_data()\n",
        "    train_x, _, test_x = normalize(train_x, train_x, test_x)    \n",
        "\n",
        "    test_y  = test_y.flatten()\n",
        "    train_y = train_y.flatten()\n",
        "    num_class = (np.unique(train_y)).shape[0]\n",
        "\n",
        "    # Pad 1 as the third feature of train_x and test_x\n",
        "    train_x = add_one(train_x) \n",
        "    test_x = add_one(test_x)\n",
        "\n",
        "    # Define hyper-parameters and train-related parameters\n",
        "    cfg = Config(num_epoch=1000, learning_rate=0.001, num_train=train_x.shape[0])\n",
        "\n",
        "    # Create NN classifier\n",
        "    num_hidden_nodes = 100\n",
        "    num_hidden_nodes_2 = 100\n",
        "    num_hidden_nodes_3 = 100\n",
        "    net = NeuralNet(num_class, cfg.reg)\n",
        "    net.add_linear_layer((train_x.shape[1],num_hidden_nodes), 'relu')\n",
        "    net.add_linear_layer((num_hidden_nodes, num_hidden_nodes_2), 'relu')\n",
        "    net.add_linear_layer((num_hidden_nodes_2, num_hidden_nodes_3), 'relu')\n",
        "    net.add_linear_layer((num_hidden_nodes_3, num_class), 'softmax')\n",
        "    \n",
        "    #Sanity check - train in small number of samples to see the overfitting problem- the loss value should decrease rapidly\n",
        "    #cfg.num_train = 500\n",
        "    #batch_train(net, train_x, train_y, cfg)\n",
        "\n",
        "    #Batch training - train all dataset\n",
        "    #batch_train(net, train_x, train_y, cfg)\n",
        "\n",
        "    #Minibatch training - training dataset using Minibatch approach\n",
        "    minibatch_train(net, train_x, train_y, cfg)\n",
        "    \n",
        "    y_hat = net.forward(test_x)[-1]\n",
        "    test(y_hat, test_y)\n",
        "\n",
        "    metrics = confusion_matrix(test_y, net.predict(test_x))\n",
        "    print(\"Accuracy: \")\n",
        "    print(metrics.trace()/test_y.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    np.random.seed(2017)\n",
        "    \n",
        "    #numerical check for your layer feedforward and backpropagation\n",
        "    your_layer = Layer((60, 100), 'sigmoid')\n",
        "    unit_test_layer(your_layer)\n",
        "\n",
        "    plt.ion()\n",
        "    bat_classification()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOzi78Qw5Hen"
      },
      "source": [
        "### 3.4.2 MNIST Classification <a id='C3_4_2'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mnist_classification():\n",
        "    # Load data from file\n",
        "    # Make sure that fashion-mnist/*.gz is in data/\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data(1)\n",
        "    train_x, val_x, test_x = normalize(train_x, train_x, test_x)    \n",
        "\n",
        "    num_class = (np.unique(train_y)).shape[0]\n",
        "\n",
        "    # Pad 1 as the third feature of train_x and test_x\n",
        "    train_x = add_one(train_x)\n",
        "    val_x = add_one(val_x)\n",
        "    test_x = add_one(test_x)\n",
        "\n",
        "    # Define hyper-parameters and train-related parameters\n",
        "    cfg = Config(num_epoch=300, learning_rate=0.001, batch_size=200, num_train=train_x.shape, visualize=False)\n",
        "\n",
        "    # Create NN classifier\n",
        "    num_hidden_nodes = 100\n",
        "    num_hidden_nodes_2 = 100\n",
        "    num_hidden_nodes_3 = 100\n",
        "    net = NeuralNet(num_class, cfg.reg)\n",
        "    net.add_linear_layer((train_x.shape[1],num_hidden_nodes), 'relu')\n",
        "    net.add_linear_layer((num_hidden_nodes, num_hidden_nodes_2), 'relu')\n",
        "    net.add_linear_layer((num_hidden_nodes_2, num_hidden_nodes_3), 'relu')\n",
        "    net.add_linear_layer((num_hidden_nodes_3, num_class), 'softmax')\n",
        "     \n",
        "    #Minibatch training - training dataset using Minibatch approach\n",
        "    minibatch_train(net, train_x, train_y, cfg)\n",
        "    \n",
        "    y_hat = net.forward(test_x)[-1]\n",
        "    test(y_hat, test_y)\n",
        "\n",
        "    metrics = confusion_matrix(test_y, net.predict(test_x))\n",
        "    print(\"Accuracy: \")\n",
        "    print(metrics.trace()/test_y.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    np.random.seed(2017)\n",
        "    \n",
        "    #numerical check for your layer feedforward and backpropagation\n",
        "    your_layer = Layer((60, 100), 'sigmoid')\n",
        "    unit_test_layer(your_layer)\n",
        "\n",
        "    plt.ion()\n",
        "    mnist_classification()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukKgfwZtRH8y"
      },
      "source": [
        "## 4. Sử dụng Tensorflow để huấn luyện mạng NN [TODO 1.7]<a id='C4'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.enable_eager_execution()\n",
        "tf.keras.backend.clear_session()\n",
        "import matplotlib.pyplot as plt\n",
        "from util import * \n",
        "from dnn_np import test\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKFoBSRRNQw"
      },
      "source": [
        "#### TODO 1.7: Tensorflow for Bat classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bat_classification():\n",
        "    # Load data from file\n",
        "    # Make sure that bat.dat is in data/\n",
        "    train_x, train_y, test_x, test_y = get_bat_data()\n",
        "    train_x, _, test_x = normalize(train_x, train_x, test_x)    \n",
        "\n",
        "    test_y  = test_y.flatten().astype(np.int32)\n",
        "    train_y = train_y.flatten().astype(np.int32)\n",
        "    num_class = (np.unique(train_y)).shape[0]\n",
        " \n",
        "    # DNN parameters\n",
        "    hidden_layers = [100, 100, 100]\n",
        "    learning_rate = 0.1\n",
        "    batch_size = 200\n",
        "    steps = 2000\n",
        "   \n",
        "    # Specify that all features have real-value data\n",
        "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[train_x.shape[1]])]\n",
        "\n",
        "\n",
        "    # Available activition functions\n",
        "    # https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_\n",
        "    # tf.nn.relu\n",
        "    # tf.nn.elu\n",
        "    # tf.nn.sigmoid\n",
        "    # tf.nn.tanh\n",
        "    activation = tf.nn.relu\n",
        "    \n",
        "    # [TODO 1.7] Create a neural network and train it using estimator\n",
        "\n",
        "    # Some available gradient descent optimization algorithms\n",
        "    # https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
        "    # tf.train.GradientDescentOptimizer\n",
        "    # tf.train.AdadeltaOptimizer\n",
        "    # tf.train.AdagradOptimizer\n",
        "    # tf.train.AdagradDAOptimizer\n",
        "    # tf.train.MomentumOptimizer\n",
        "    # tf.train.AdamOptimizer\n",
        "    # tf.train.FtrlOptimizer\n",
        "    # tf.train.ProximalGradientDescentOptimizer\n",
        "    # tf.train.ProximalAdagradOptimizer\n",
        "    # tf.train.RMSPropOptimizer\n",
        "    # Create optimizer\n",
        "    optimizer = tf.train.ProximalAdagradOptimizer(\n",
        "                                    learning_rate=learning_rate, \n",
        "                                    l2_regularization_strength=0.001)\n",
        "\n",
        "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "    # optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.005)\n",
        "    \n",
        "    # build a deep neural network\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier\n",
        "    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, \n",
        "                                    hidden_units= hidden_layers,\n",
        "                                    n_classes=num_class,\n",
        "                                    optimizer= optimizer,\n",
        "                                    activation_fn= activation,\n",
        "                                    model_dir=\"problem1-estimator\")\n",
        "    \n",
        "    # Define the training inputs\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn\n",
        "    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "                                    x={\"x\": train_x},\n",
        "                                    y=train_y,\n",
        "                                    num_epochs=None, # None will run forever.\n",
        "                                    batch_size = batch_size,\n",
        "                                    shuffle=True)\n",
        "    \n",
        "    # Train model.\n",
        "    classifier.train(\n",
        "        input_fn=train_input_fn,\n",
        "        steps=steps)\n",
        "    \n",
        "    # Define the test inputs\n",
        "    test_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "                                    x={\"x\": test_x},\n",
        "                                    y=test_y,\n",
        "                                    num_epochs=1,\n",
        "                                    shuffle=False)\n",
        "    \n",
        "    # Evaluate accuracy. \n",
        "    predict_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "      x={\"x\": test_x},\n",
        "      num_epochs=1,\n",
        "      shuffle=False)\n",
        "\n",
        "    y_hat = classifier.predict(input_fn=predict_input_fn)\n",
        "    y_hat = list(y_hat)\n",
        "    y_hat = np.asarray([int(x['classes'][0]) for x in y_hat]) \n",
        "    test(y_hat, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    np.random.seed(2017) \n",
        "\n",
        "    plt.ion()\n",
        "    bat_classification()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO 1.7: Tensorflow for MNIST classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mnist_classification():\n",
        "    # Load data from file\n",
        "    # Make sure that fashion-mnist/*.gz is in data/\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data(1)\n",
        "\n",
        "    train_x, val_x, test_x = normalize(train_x, train_x, test_x)    \n",
        "\n",
        "    train_y = train_y.flatten().astype(np.int32)\n",
        "    val_y = val_y.flatten().astype(np.int32)\n",
        "    test_y = test_y.flatten().astype(np.int32)\n",
        "    num_class = (np.unique(train_y)).shape[0]\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    # DNN parameters\n",
        "    hidden_layers = [100, 100, 100]\n",
        "    learning_rate = 0.1\n",
        "    batch_size = 200\n",
        "    steps = 500\n",
        "   \n",
        "    # Specify that all features have real-value data\n",
        "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[train_x.shape[1]])]\n",
        "\n",
        "\n",
        "    # Choose activation function\n",
        "    activation = tf.nn.relu\n",
        "    \n",
        "    # Some available gradient descent optimization algorithms \n",
        "    # TODO: [YC1.7] Create optimizer\n",
        "    optimizer = tf.train.ProximalAdagradOptimizer(\n",
        "                                    learning_rate=learning_rate, \n",
        "                                    l2_regularization_strength=0.001)\n",
        "    \n",
        "    # build a deep neural network\n",
        "    classifier = tf.estimator.DNNClassifier(\n",
        "                                    feature_columns=feature_columns,\n",
        "                                    hidden_units= hidden_layers,\n",
        "                                    n_classes=num_class,\n",
        "                                    optimizer= optimizer,\n",
        "                                    activation_fn= activation,\n",
        "                                    model_dir=\"problem2-estimator\") \n",
        "    \n",
        "    # Define the training inputs\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn\n",
        "    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "                                    x={\"x\": train_x},\n",
        "                                    y=train_y,\n",
        "                                    num_epochs=None, # None will run forever.\n",
        "                                    batch_size = batch_size,\n",
        "                                    shuffle=True)\n",
        "    \n",
        "    # Train model.\n",
        "    classifier.train(\n",
        "        input_fn=train_input_fn,\n",
        "        steps=steps)\n",
        "    \n",
        "    # Define the test inputs\n",
        "    test_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "                                    x={\"x\": test_x},\n",
        "                                    y=test_y,\n",
        "                                    num_epochs=1,\n",
        "                                    shuffle=False)\n",
        "    \n",
        "    # Evaluate accuracy. \n",
        "    predict_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "      x={\"x\": test_x},\n",
        "      num_epochs=1,\n",
        "      shuffle=False)\n",
        "    y_hat = classifier.predict(input_fn=predict_input_fn)\n",
        "    y_hat = list(y_hat)\n",
        "    y_hat = np.asarray([int(x['classes'][0]) for x in y_hat]) \n",
        "    test(y_hat, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    np.random.seed(2017) \n",
        "\n",
        "    plt.ion()\n",
        "    mnist_classification()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 Kết quả <a id='C5'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlidH1M6jjOe"
      },
      "source": [
        "### 5.1. Bat Classification <a id='*C5_1*'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Numpy:\n",
        "- Epoch 1000: loss is 0.15591\n",
        "- Confusion matrix:\n",
        " $$\\begin{bmatrix} 0.98 & 0.02 & 0. \\\\ 0.06 & 0.9 & 0.05 \\\\ 0. & 0.04 & 0.96 \\end{bmatrix}$$\n",
        "- Diagonal values:\n",
        " $$\\begin{bmatrix} 0.98 & 0.9 & 0.96 \\end{bmatrix}$$\n",
        "- Accuracy: 0.947\n",
        "\n",
        "#### Tensorflow:\n",
        "- INFO:tensorflow:Loss for final step: 22.787983.\n",
        "- Confusion matrix:\n",
        " $$\\begin{bmatrix} 0.97 & 0.03 & 0. \\\\ 0.04 & 0.95 & 0.01 \\\\ 0. & 0.12 & 0.88 \\end{bmatrix}$$\n",
        "- Diagonal values:\n",
        "$$\\begin{bmatrix} 0.97 & 0.95 & 0.88 \\end{bmatrix}$$\n",
        "- Accuracy: 0.933\n",
        "\n",
        "Kết quả khi sử dụng numpy có vẻ ổn định và chính xác hơn khi sử dụng Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2. MNIST Classification <a id='*C5_2*'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Numpy:\n",
        "- Epoch 300: loss is 0.07380\n",
        "- Confusion matrix:\n",
        "$$\\begin{bmatrix} 0.79 & 0.01 & 0.03 & 0.02 & 0.01 & 0. & 0.14 & 0. & 0.01 & 0. \n",
        "    \\\\ 0. & 0.97 & 0. & 0.02 & 0.01 & 0. & 0. & 0. & 0. & 0.\n",
        "    \\\\ 0.02 & 0.01 & 0.79 & 0.01 & 0.1 & 0. & 0.07 0. & 0. & 0.\n",
        "    \\\\ 0.03 & 0.02 & 0.01 & 0.86 & 0.03 & 0. & 0.04 & 0. & 0.01 & 0.\n",
        "    \\\\ 0. & 0. & 0.1 & 0.03 & 0.79 & 0. & 0.07 & 0. & 0.01 & 0.\n",
        "    \\\\ 0. & 0. & 0. & 0. & 0. & 0.96 & 0. & 0.02 & 0.01 & 0.01\n",
        "    \\\\ 0.11 & 0. & 0.08 & 0.02 & 0.06 & 0. & 0.7 & 0. & 0.01 & 0.\n",
        "    \\\\ 0. & 0. & 0. & 0. & 0. & 0.02 & 0. & 0.95 & 0. & 0.03\n",
        "    \\\\ 0.01 & 0. & 0. & 0. & 0. & 0. & 0.02 & 0. & 0.96 & 0.\n",
        "    \\\\ 0. & 0. & 0. & 0. & 0. & 0.01 & 0. & 0.04 & 0. & 0.95\n",
        "    \\end{bmatrix}$$\n",
        "- Diagonal values:\n",
        "$$\\begin{bmatrix} 0.79 & 0.97 & 0.79 & 0.86 & 0.79 & 0.96 & 0.7 & 0.95 & 0.96 & 0.95 \\end{bmatrix}$$\n",
        "- Accuracy: \n",
        "0.8735\n",
        "#### Tensorflow:\n",
        "- INFO:tensorflow:Loss for final step: 66.0305.\n",
        "- Confusion matrix:\n",
        " $$\\begin{bmatrix} 0.77 & 0. & 0.02 & 0.08 & 0. & 0. & 0.12 & 0. & 0.01 & 0. \n",
        "    \\\\ 0. & 0.96 & 0. & 0.03 & 0.01 & 0. & 0. & 0. & 0. & 0.\n",
        "    \\\\ 0.01 & 0. & 0.82 & 0.02 & 0.1 & 0. & 0.05 & 0. & 0.01 & 0.\n",
        "    \\\\ 0.01 & 0.01 & 0.01 & 0.9 & 0.04 & 0. & 0.02 & 0. & 0.01 & 0.\n",
        "    \\\\ 0. & 0. & 0.15 & 0.04 & 0.76 & 0. & 0.04 & 0. & 0. & 0.\n",
        "    \\\\ 0. & 0. & 0. & 0. & 0. & 0.93 & 0. & 0.04 & 0.01 & 0.02\n",
        "    \\\\ 0.12 & 0. & 0.13 & 0.06 & 0.08 & 0. & 0.59 & 0. & 0.01 & 0.\n",
        "    \\\\ 0. & 0. & 0. & 0. & 0. & 0.02 & 0. & 0.96 & 0. & 0.02\n",
        "    \\\\ 0. & 0. & 0. & 0.01 & 0.01 & 0. & 0.01 & 0. & 0.97 & 0.\n",
        "    \\\\ 0. & 0. & 0. & 0. & 0. & 0. & 0. & 0.06 & 0. & 0.94\n",
        "    \\end{bmatrix}$$\n",
        "- Diagonal values:\n",
        "$$\\begin{bmatrix} 0.77 & 0.96 & 0.82 & 0.9 & 0.76 & 0.93 & 0.59 & 0.96 & 0.97 & 0.94 \\end{bmatrix}$$\n",
        "- Accuracy: 0.86\n",
        "\n",
        "Kết quả khi sử dụng numpy có vẻ ổn định và chính xác hơn khi sử dụng Tensorflow"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MSU-sZg3oM7N",
        "GwSNZlg2qFCv",
        "enKFoBSRRNQw",
        "WkG26LWSSdDg",
        "69rProsfTYWS",
        "Yy_5nMf7mOTW"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "31671a60cee805c34c73116577b485118ff3a75c458d3004d49632c19702ac60"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
