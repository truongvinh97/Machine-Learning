{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear Regression\n",
    "> In this exercise, I will implement the Linear Regression with one variable and work with example data.\n",
    "> * Input: Square feet (X)\n",
    "> * Output: House price in $1000s (Y)\n",
    "> * Number of training examples: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [1. Plotting the data](#C1)\n",
    "* [2. Gradient Descent](#C2)\n",
    "    * [2.1. Equations](#C2_1)\n",
    "    * [2.2. Implementation](#C2_2)\n",
    "    * [2.3. Cost function $J(\\theta)$](#C2_3)\n",
    "    * [2.4. Gradient Descent](#C2_4)\n",
    "* [3. Visualizing $J(\\theta)$](#C3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plotting the data <a id='C1'></a>\n",
    "This chapter show the graph of example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('example_data.txt', sep=',', header=None)\n",
    "df.columns = ['House_Price', 'Square_Feet']\n",
    "pd.DataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='Square_Feet', y='House_Price', data=df, s = 200)\n",
    "ax.set(xlabel='Square Feet', ylabel='House Price ($1000s)', title='Scatter plot of example data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent <a id='C2'></a>\n",
    "This chapter fit the linear regression parameters $\\theta$ to the dataset using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Equations <a id='C2_1'></a>\n",
    "The hypothesis of linear regression is:\n",
    "$$h_{\\theta}(x)=\\theta_{0} + \\theta_{1} x_{1}$$\n",
    "The objective of linear regression is to minimize the cost function (Root Mean Square Error RMSE):\n",
    "$$J(\\theta_{0},\\theta_{1})=\\frac{1}{2m} \\Sigma(h_{\\theta}(x^{(i)}) - y^{(i)})^2$$\n",
    "To minimize the cost of $J(\\theta)$ we will use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update:\n",
    "\n",
    "$$\\theta_{j} := \\theta_{j} - \\alpha \\frac1m \\Sigma(h_{\\theta}(x^i) - y^i) x_{j}^{(i)}$$\n",
    "(simultaneously update $\\theta_{j}$ for all $j$ ). With each step of gradient descent, your parameters $\\theta_{j}$ come closer to the optimal values that will achieve the lowest cost $J(\\theta)$ .\n",
    "\n",
    "Note: $\\Sigma$ (with $i=1$ to $m$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Implementation <a id='C2_2'></a>\n",
    "This chapter describes how to find parameter to implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df.shape[0]\n",
    "print(\"m is: %d\\n\"%(m))\n",
    "\n",
    "X = np.hstack((np.ones((m,1)), df.Square_Feet.values.reshape(-1,1)))\n",
    "y = np.array(df.House_Price.values).reshape(-1,1)\n",
    "theta = np.zeros(shape=(X.shape[1],1))\n",
    "print(\"theta is:\\n\",theta)\n",
    "\n",
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cost function $J(\\theta)$ <a id='C2_3'></a>\n",
    "This chapter describes how to compute the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(X, y, theta):\n",
    "    m = y.shape[0]\n",
    "    h = X.dot(theta)\n",
    "    J = (1/(2*m)) * (np.sum((h - y)**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = compute_cost_function(X, y, theta)\n",
    "print('With theta = [0 ; 0]\\nCost computed =', J)\n",
    "print('Expected cost value (approx) 32.07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = compute_cost_function(X, y, [[-1],[2]])\n",
    "print('With theta = [-1 ; 2]\\nCost computed =', J)\n",
    "print('Expected cost value (approx) 54.24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Gradient Descent <a id='C2_4'></a>\n",
    "Gradient descent is a generic optimization algorithm that measures the local gradient of the cost function with regards to the parameter $\\theta$ and goes in the direction of descending gradient.\n",
    "* Learning rate to small: slow gradient descent\n",
    "* Learning rate to large: gradient descent can overshoot the minimum, may fail to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = y.shape[0]\n",
    "    J_history = np.zeros(shape=(num_iters, 1))\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        h = X.dot(theta)\n",
    "        diff_hy = h - y\n",
    "\n",
    "        delta = (1/m) * (diff_hy.T.dot(X))\n",
    "        theta = theta - (alpha * delta.T)\n",
    "        J_history[i] = compute_cost_function(X, y, theta)\n",
    "\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, _ = gradient_descent(X, y, theta, alpha, iterations)\n",
    "print('Theta found by gradient descent:\\n', theta)\n",
    "print('Expected theta values (approx)\\n -3.6303\\n  1.1664')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the linear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='Square_Feet', y='House_Price', data=df)\n",
    "plt.plot(X[:,1], X.dot(theta), color='r')\n",
    "ax.set(xlabel='Square Feet', ylabel='House Price ($1000s)', title='Training data with linear regression fit');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([1, 2000]).dot(theta)\n",
    "f'For Square Feet = 2000, we predict a House Price of {y_pred[0]*1000}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([1, 1800]).dot(theta)\n",
    "f'For Square Feet = 1800, we predict a House Price of {y_pred[0]*1000}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing $J(\\theta)$ <a id='C3'></a>\n",
    "The cost function $J(\\theta)$ is bowl-shaped and has a global mininum. This minimum is the optimal point for \n",
    "$\\theta_{0}$ and $\\theta_{1}$, and each step of gradient descent moves closer to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_vals = np.zeros(shape=(len(theta0_vals), len(theta1_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(theta0_vals)):\n",
    "    for j in range(0, len(theta1_vals)):\n",
    "        J_vals[i,j] = compute_cost_function(X, y, [[theta0_vals[i]], [theta1_vals[j]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.contour(theta0_vals, theta1_vals, np.transpose(J_vals), levels=np.logspace(-2,3,20))\n",
    "plt.plot(theta[0,0], theta[1,0], marker='x', color='r');\n",
    "plt.xlabel(r'$\\theta_0$');\n",
    "plt.ylabel(r'$\\theta_1$');\n",
    "plt.title('Contour, showing minimum');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31671a60cee805c34c73116577b485118ff3a75c458d3004d49632c19702ac60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
