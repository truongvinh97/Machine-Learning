{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5cdd104",
   "metadata": {},
   "source": [
    "This file is for binary classification using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741887e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from util import get_vehicle_data\n",
    "from logistic_np import *\n",
    "\n",
    "# [TODO 1.11] Create TF placeholders to feed train_x and train_y when training\n",
    "def create_placeholders():\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    x = tf.compat.v1.placeholder(tf.float32, name='x')\n",
    "    y = tf.compat.v1.placeholder(tf.float32, name='y')\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# [TODO 1.12] Create weights (W) using TF variables\n",
    "def create_w_variable(w_shape):\n",
    "    w = tf.Variable(np.random.normal(0, np.sqrt(2./np.sum(w_shape)), w_shape) , name = \"w\" , dtype=tf.float32)\n",
    "\n",
    "    return w\n",
    "\n",
    "# [TODO 1.13] Create a feed-forward operator\n",
    "def tf_feed_forward(x, w):\n",
    "    pred = 1.0 / (1.0 + tf.exp(-tf.matmul(x, w)))\n",
    "\n",
    "    return pred\n",
    "\n",
    "# [TODO 1.14] Write the cost function\n",
    "def tf_cost(y, pred):\n",
    "    cost = -tf.reduce_mean((y*tf.math.log(pred) + (1-y)*tf.math.log(1-pred)))\n",
    "\n",
    "    return cost\n",
    "\n",
    "# [TODO 1.15] Create an SGD optimizer\n",
    "def tf_optimizer(learning_rate, cost):\n",
    "    tf.compat.v1.disable_v2_behavior()\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "# [TODO 1.16] Compute loss and update weights here\n",
    "def tf_update_weight(session, cost,x,y, optimizer, train_x, train_y):\n",
    "    session.run(optimizer, feed_dict={x: train_x, y: train_y})\n",
    "    loss = session.run(cost, feed_dict={x: train_x, y: train_y})\n",
    "\n",
    "    return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    normalize_method = \"all_pixel\"\n",
    "\n",
    "    np.random.seed(2018)\n",
    "    tf.random.set_seed(2018)\n",
    "\n",
    "    # Load data from file\n",
    "    # Make sure that vehicles.dat is in data/\n",
    "    train_x, train_y, test_x, test_y = get_vehicle_data()\n",
    "    num_train = train_x.shape[0]\n",
    "    num_test = test_x.shape[0]\n",
    "\n",
    "    #generate_unit_testcase(train_x.copy(), train_y.copy())\n",
    "    # logistic_unit_test()\n",
    "\n",
    "    # Normalize our data: choose one of the two methods before training\n",
    "    #train_x, test_x = normalize_all_pixel(train_x, test_x)\n",
    "    if normalize_method == \"all_pixel\":\n",
    "        train_x, test_x = normalize_all_pixel(train_x, test_x) \n",
    "    else:\n",
    "        train_x, test_x = normalize_per_pixel(train_x, test_x) \n",
    "\n",
    "    # Reshape our data\n",
    "    # train_x: shape=(2400, 64, 64) -> shape=(2400, 64*64)\n",
    "    # test_x: shape=(600, 64, 64) -> shape=(600, 64*64)\n",
    "    train_x = reshape2D(train_x)\n",
    "    test_x = reshape2D(test_x)\n",
    "\n",
    "    # Pad 1 as the last feature of train_x and test_x\n",
    "    train_x = add_one(train_x)\n",
    "    test_x = add_one(test_x)\n",
    "\n",
    "    # [TODO 1.11] Create TF placeholders to feed train_x and train_y when training\n",
    "    x,y = create_placeholders()\n",
    "\n",
    "    # [TODO 1.12] Create weights (W) using TF variables\n",
    "    w_shape = (train_x.shape[1],1)\n",
    "    w = create_w_variable(w_shape)\n",
    "\n",
    "    # [TODO 1.13] Create a feed-forward operator\n",
    "    pred = tf_feed_forward(x, w)\n",
    "\n",
    "    # [TODO 1.14] Write the cost function\n",
    "    #cost = -tf.reduce_sum(y*tf.log(pred)+(1-y)*tf.log(1-pred))/num_train\n",
    "    cost = tf_cost(y, pred) \n",
    "\n",
    "    # Define hyper-parameters and train-related parameters\n",
    "    num_epoch = 1000\n",
    "    learning_rate = 0.01\n",
    "#    momentum_rate = 0.9\n",
    "\n",
    "    # [TODO 1.15] Create an SGD optimizer\n",
    "    optimizer = tf_optimizer(learning_rate, cost)\n",
    "\n",
    "    # Some meta parameters\n",
    "    epochs_to_draw = 100\n",
    "    all_loss = []\n",
    "    plt.ion()\n",
    "\n",
    "    # Start training\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        for e in range(num_epoch):\n",
    "            tic = time.time()\n",
    "            # [TODO 1.16] Compute loss and update weights here\n",
    "            loss = tf_update_weight(sess, cost,x,y, optimizer, train_x, train_y)\n",
    "            # Update weights...\n",
    "\n",
    "            all_loss.append(loss)\n",
    "\n",
    "            if (e % epochs_to_draw == epochs_to_draw-1):\n",
    "                plot_loss(all_loss)\n",
    "                plt.show()\n",
    "                plt.pause(0.1)\n",
    "                print(\"Epoch %d: loss is %.5f\" % (e+1, loss))\n",
    "            toc = time.time()\n",
    "            print(toc-tic)\n",
    "        y_hat = sess.run(pred, feed_dict={x: test_x})\n",
    "        test(y_hat, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31671a60cee805c34c73116577b485118ff3a75c458d3004d49632c19702ac60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
