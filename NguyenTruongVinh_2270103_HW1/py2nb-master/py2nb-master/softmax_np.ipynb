{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2c1f49",
   "metadata": {},
   "source": [
    "This file is for fashion mnist classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b70bb4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import get_mnist_data\n",
    "from logistic_np import add_one, LogisticClassifier\n",
    "import time\n",
    "#import pdb\n",
    "\n",
    "\n",
    "class SoftmaxClassifier(LogisticClassifier):\n",
    "    def __init__(self, w_shape):\n",
    "        \"\"\"__init__\n",
    "\n",
    "        :param w_shape: create w with shape w_shape using normal distribution\n",
    "        \"\"\"\n",
    "        super(SoftmaxClassifier, self).__init__(w_shape)\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"softmax\n",
    "        Compute softmax on the second axis of x\n",
    "\n",
    "        :param x: input\n",
    "        \"\"\"\n",
    "        # [TODO 2.3]\n",
    "        # Compute softmax\n",
    "        max_x = np.max(x, axis=1, keepdims=True)\n",
    "        x -= max_x\n",
    "        x_exp = np.exp(x)\n",
    "        result = x_exp / np.sum(x_exp, axis=1, keepdims=True)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"feed_forward\n",
    "        This function compute the output of your softmax regression model\n",
    "\n",
    "        :param x: input\n",
    "        \"\"\"\n",
    "        # [TODO 2.3]\n",
    "        # Compute a feed forward pass\n",
    "        x_out = np.dot(x, self.w)\n",
    "        result = self.softmax(x_out)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        \"\"\"compute_loss\n",
    "        Compute the loss using y (label) and y_hat (predicted class)\n",
    "\n",
    "        :param y:  the label, the actual class of the samples\n",
    "        :param y_hat: the class probabilities of all samples in our data\n",
    "        \"\"\"\n",
    "        # [TODO 2.4]\n",
    "        # Compute categorical loss\n",
    "        loss = -np.log(y_hat) \n",
    "        loss = np.sum(loss*y, axis=1)\n",
    "        loss = np.mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def get_grad(self, x, y, y_hat):\n",
    "        \"\"\"get_grad\n",
    "        Compute and return the gradient of w\n",
    "\n",
    "        :param loss: computed loss between y_hat and y in the train dataset\n",
    "        :param y_hat: predicted y\n",
    "        \"\"\" \n",
    "        # [TODO 2.5]\n",
    "        # Compute gradient of the loss function with respect to w\n",
    "        num_x = x.shape[0]\n",
    "        w_grad = np.dot(x.T, -(y - y_hat))/num_x\n",
    "\n",
    "        return w_grad\n",
    "\n",
    "\n",
    "def plot_loss(train_loss, val_loss):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.plot(train_loss, color='b')\n",
    "    plt.plot(val_loss, color='g')\n",
    "\n",
    "\n",
    "def draw_weight(w):\n",
    "    label_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    plt.figure(2, figsize=(8, 6))\n",
    "    plt.clf()\n",
    "    w = w[0:(28*28),:].reshape(28, 28, 10)\n",
    "    for i in range(10):\n",
    "        ax = plt.subplot(3, 4, i+1)\n",
    "        plt.imshow(w[:,:,i], interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        ax.set_title(label_names[i])\n",
    "\n",
    "\n",
    "def normalize(train_x, val_x, test_x):\n",
    "    \"\"\"normalize\n",
    "    This function computes train mean and standard deviation on all pixels then applying data scaling on train_x, val_x and test_x using these computed values\n",
    "    Note that in this classification problem, the data is already flatten into a shape of (num_samples, image_width*image_height)\n",
    "\n",
    "    :param train_x: train images, shape=(num_train, image_height*image_width)\n",
    "    :param val_x: validation images, shape=(num_val, image_height*image_width)\n",
    "    :param test_x: test images, shape=(num_test, image_height*image_width)\n",
    "    \"\"\"\n",
    "    # [TODO 2.1]\n",
    "    # train_mean and train_std should have the shape of (1, 1)\n",
    "    train_mean = np.mean(train_x, axis=(0,1), dtype=np.float64, keepdims=True)\n",
    "    train_std = np.std(train_x, axis=(0,1), dtype=np.float64, keepdims=True)\n",
    "\n",
    "    train_x = (train_x-train_mean)/train_std\n",
    "    val_x = (val_x-train_mean)/train_std\n",
    "    test_x = (test_x-train_mean)/train_std\n",
    "\n",
    "    return train_x, val_x, test_x\n",
    "\n",
    "def create_one_hot(labels, num_k=10):\n",
    "    \"\"\"create_one_hot\n",
    "    This function creates a one-hot (one-of-k) matrix based on the given labels\n",
    "\n",
    "    :param labels: list of labels, each label is one of 0, 1, 2,... , num_k - 1\n",
    "    :param num_k: number of classes we want to classify\n",
    "    \"\"\"\n",
    "    # [TODO 2.2]\n",
    "    # Create the one-hot label matrix here based on labels\n",
    "    eye_mat = np.eye(num_k)\n",
    "    eye_mat = eye_mat[labels, :].astype(np.float32)\n",
    "\n",
    "    return eye_mat\n",
    "\n",
    "def is_stop_training(all_val_loss):\n",
    "    \"\"\"is_stop_training\n",
    "    Check whether training need to be stopped\n",
    "\n",
    "    :param all_val_loss: list of all validation loss values during training\n",
    "    \"\"\"\n",
    "    is_stopped = False\n",
    "    num_val_increase = 0\n",
    "    if (len(all_val_loss)<2):\n",
    "        return False\n",
    "    for i in range(len(all_val_loss)-1,0,-1):\n",
    "        if all_val_loss[i] > all_val_loss[i-1]:\n",
    "            num_val_increase += 1\n",
    "        if num_val_increase >= 4:\n",
    "            return True\n",
    "\n",
    "    return is_stopped\n",
    "\n",
    "def test(y_hat, test_y):\n",
    "    \"\"\"test\n",
    "    Compute the confusion matrix based on labels and predicted values \n",
    "\n",
    "    :param classifier: the trained classifier\n",
    "    :param y_hat: predicted probabilites, output of classifier.feed_forward\n",
    "    :param test_y: test labels\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_mat = np.zeros((10,10))\n",
    "\n",
    "    # [TODO 2.7]\n",
    "    # Compute the confusion matrix here\n",
    "\n",
    "    #confusion_mat = confusion_mat/np.sum(confusion_mat,axis=1)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "    test_y = np.argmax(test_y, axis=1)\n",
    "    confusion_mat = np.zeros((10,10))\n",
    "    for i in range(10):\n",
    "        class_i_idx = test_y == i\n",
    "        num_class_i = np.sum(class_i_idx)\n",
    "        y_hat_i = y_hat[class_i_idx]\n",
    "        for j in range(10):\n",
    "            confusion_mat[i,j] = 1.0*np.sum(y_hat_i == j)/num_class_i\n",
    "\n",
    "    return confusion_mat\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(2020)\n",
    "\n",
    "    # Load data from file\n",
    "    # Make sure that fashion-mnist/*.gz files is in data/\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()\n",
    "    num_train = train_x.shape[0]\n",
    "    num_val = val_x.shape[0]\n",
    "    num_test = test_x.shape[0]  \n",
    "\n",
    "    print(train_x.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_x.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_x.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    imgplot = plt.imshow(train_x[0].reshape(28,28))\n",
    "    plt.show()\n",
    "\n",
    "    imgplot = plt.imshow(train_x[100].reshape(28,28))\n",
    "    plt.show()\n",
    "\n",
    "    #generate_unit_testcase(train_x.copy(), train_y.copy()) \n",
    "\n",
    "    # Convert label lists to one-hot (one-of-k) encoding\n",
    "    train_y = create_one_hot(train_y)\n",
    "    val_y = create_one_hot(val_y)\n",
    "    test_y = create_one_hot(test_y)\n",
    "\n",
    "    # Normalize our data\n",
    "    train_x, val_x, test_x = normalize(train_x, val_x, test_x)\n",
    "\n",
    "    # Pad 1 as the last feature of train_x and test_x\n",
    "    train_x = add_one(train_x) \n",
    "    val_x = add_one(val_x)\n",
    "    test_x = add_one(test_x)\n",
    "\n",
    "    # Create classifier\n",
    "    num_feature = train_x.shape[1]\n",
    "    dec_classifier = SoftmaxClassifier((num_feature, 10))\n",
    "    momentum = np.zeros_like(dec_classifier.w)\n",
    "\n",
    "    # Define hyper-parameters and train-related parameters\n",
    "    num_epoch = 3340\n",
    "    learning_rate = 0.01\n",
    "    momentum_rate = 0.9\n",
    "    epochs_to_draw = 10\n",
    "    all_train_loss = []\n",
    "    all_val_loss = []\n",
    "    plt.ion()\n",
    "\n",
    "    for e in range(num_epoch):    \n",
    "        tic = time.time()\n",
    "        train_y_hat = dec_classifier.feed_forward(train_x)\n",
    "        val_y_hat = dec_classifier.feed_forward(val_x)\n",
    "\n",
    "        train_loss = dec_classifier.compute_loss(train_y, train_y_hat)\n",
    "        val_loss = dec_classifier.compute_loss(val_y, val_y_hat)\n",
    "\n",
    "        grad = dec_classifier.get_grad(train_x, train_y, train_y_hat)\n",
    "\n",
    "        # dec_classifier.numerical_check(train_x, train_y, grad)\n",
    "        # Updating weight: choose either normal SGD or SGD with momentum\n",
    "        dec_classifier.update_weight(grad, learning_rate)\n",
    "        #dec_classifier.update_weight_momentum(grad, learning_rate, momentum, momentum_rate)\n",
    "\n",
    "        all_train_loss.append(train_loss) \n",
    "        all_val_loss.append(val_loss)\n",
    "        # [TODO 2.6]\n",
    "        # Propose your own stopping condition here\n",
    "        if is_stop_training(all_val_loss):\n",
    "            break\n",
    "\n",
    "        if (e % epochs_to_draw == epochs_to_draw-1):\n",
    "            from IPython.display import clear_output\n",
    "            clear_output(wait=True)\n",
    "            plot_loss(all_train_loss, all_val_loss)\n",
    "            draw_weight(dec_classifier.w)\n",
    "            plt.show()\n",
    "            plt.pause(0.1) \n",
    "            print(\"Epoch %d: train loss: %.5f || val loss: %.5f\" % (e+1, train_loss, val_loss))\n",
    "        toc = time.time()\n",
    "        print(toc-tic)\n",
    "\n",
    "    y_hat = dec_classifier.feed_forward(test_x)\n",
    "    np.set_printoptions(precision=2)\n",
    "    confusion_mat = test(y_hat, test_y)\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_mat)\n",
    "    print('Diagonal values:')\n",
    "    print(confusion_mat.flatten()[0::11])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
