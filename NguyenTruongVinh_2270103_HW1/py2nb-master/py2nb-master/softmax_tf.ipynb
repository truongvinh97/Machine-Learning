{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67caa924",
   "metadata": {},
   "source": [
    "This file is for multiclass fashion-mnist classification using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d52d3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from util import get_mnist_data\n",
    "from logistic_np import add_one\n",
    "from softmax_np import *\n",
    "import time\n",
    "\n",
    "# [TODO 2.8] Create TF placeholders to feed train_x and train_y when training\n",
    "def tf_softmax_placeholder():\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    x = tf.compat.v1.placeholder(tf.float32, name='x')\n",
    "    y = tf.compat.v1.placeholder(tf.float32, name='y')\n",
    "\n",
    "    return x,y\n",
    "\n",
    "# [TODO 2.8] Create weights (W) using TF variables\n",
    "def tf_softmax_w_variable(w_shape):\n",
    "    w = tf.Variable(np.random.normal(0, np.sqrt(2./np.sum(w_shape)), w_shape) , name = \"w\" , dtype=tf.float32)\n",
    "\n",
    "    return w\n",
    "\n",
    "# [TODO 2.8] Create an SGD optimizer\n",
    "def tf_softmax_create_optimizer(learning_rate, cost):\n",
    "    tf.compat.v1.disable_v2_behavior()\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "# [TODO 2.8] Compute losses and update weights here\n",
    "def tf_softmax_update_weight(sess, optimizer, cost,x, y, train_x, train_y, val_x, val_y):\n",
    "    sess.run(optimizer, feed_dict={x: train_x, y: train_y})\n",
    "    train_loss = sess.run(cost, feed_dict={x: train_x, y: train_y})\n",
    "    val_loss = sess.run(cost, feed_dict={x: val_x, y: val_y})\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# [TODO 2.9] Create a feed-forward operator \n",
    "def tf_softmax_feed_forward(x, w):\n",
    "    pred = tf.matmul(x, w)\n",
    "    pred_max = tf.reduce_max(pred, 1, True)\n",
    "    pred -= pred_max\n",
    "    exp_pred = tf.exp(pred)\n",
    "    pred = exp_pred / tf.reduce_sum(exp_pred, 1, True)\n",
    "\n",
    "    return pred\n",
    "\n",
    "# [TODO 2.10] Write the cost function\n",
    "def tf_softmax_cost(pred, y):\n",
    "    cost = -tf.reduce_mean(tf.reduce_sum(y*tf.math.log(pred), 1))\n",
    "\n",
    "    return cost\n",
    "\n",
    "# [TODO 2.11] Define your own stopping condition here \n",
    "def is_stop_training(all_val_loss):\n",
    "    is_stopped = False\n",
    "    num_val_increase = 0\n",
    "    if (len(all_val_loss)<2):\n",
    "        return False\n",
    "    for i in range(len(all_val_loss)-1,0,-1):\n",
    "        if all_val_loss[i] > all_val_loss[i-1]:\n",
    "            num_val_increase += 1\n",
    "        if num_val_increase >= 4:\n",
    "            return True\n",
    "\n",
    "    return is_stopped\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(2020)\n",
    "    tf.random.set_seed(2020)\n",
    "    from IPython.display import clear_output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Load data from file\n",
    "    # Make sure that fashion-mnist/*.gz files is in data/\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()\n",
    "    num_train = train_x.shape[0]\n",
    "    num_val = val_x.shape[0]\n",
    "    num_test = test_x.shape[0]  \n",
    "\n",
    "    # generate_unit_testcase(train_x.copy(), train_y.copy()) \n",
    "\n",
    "    # Convert label lists to one-hot (one-of-k) encoding\n",
    "    train_y = create_one_hot(train_y)\n",
    "    val_y = create_one_hot(val_y)\n",
    "    test_y = create_one_hot(test_y)\n",
    "\n",
    "    # Normalize our data\n",
    "    train_x, val_x, test_x = normalize(train_x, val_x, test_x)\n",
    "\n",
    "    # Pad 1 as the last feature of train_x and test_x\n",
    "    train_x = add_one(train_x) \n",
    "    val_x = add_one(val_x)\n",
    "    test_x = add_one(test_x)\n",
    "\n",
    "    # [TODO 2.8] Create TF placeholders to feed train_x and train_y when training\n",
    "    x, y = tf_softmax_placeholder()\n",
    "\n",
    "    # [TODO 2.8] Create weights (W) using TF variables \n",
    "    w_shape = (train_x.shape[1],10)\n",
    "    w = tf_softmax_w_variable(w_shape)\n",
    "\n",
    "    # [TODO 2.9] Create a feed-forward operator \n",
    "    pred = tf_softmax_feed_forward(x, w)\n",
    "\n",
    "    # [TODO 2.10] Write the cost function\n",
    "    cost = tf_softmax_cost(pred, y) \n",
    "\n",
    "    # Define hyper-parameters and train-related parameters\n",
    "    #num_epoch = 10000\n",
    "    num_epoch = 3340\n",
    "    learning_rate = 0.01    \n",
    "\n",
    "    # [TODO 2.8] Create an SGD optimizer\n",
    "    optimizer = tf_softmax_create_optimizer(learning_rate,cost)\n",
    "\n",
    "    # Some meta parameters\n",
    "    epochs_to_draw = 10\n",
    "    all_train_loss = []\n",
    "    all_val_loss = []\n",
    "    plt.ion()\n",
    "    num_val_increase = 0\n",
    "\n",
    "    # Start training\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        for e in range(num_epoch):\n",
    "            tic = time.time()\n",
    "            # [TODO 2.8] Compute losses and update weights here\n",
    "            train_loss, val_loss = tf_softmax_update_weight(sess, optimizer, cost,x,y, train_x, train_y, val_x, val_y)\n",
    "\n",
    "            train_loss = sess.run(cost, feed_dict={x:train_x, y:train_y}) \n",
    "            val_loss = sess.run(cost, feed_dict={x:val_x, y:val_y}) \n",
    "            # Update weights\n",
    "            sess.run(optimizer, feed_dict={x: train_x, y: train_y})\n",
    "            all_train_loss.append(train_loss)\n",
    "            all_val_loss.append(val_loss)\n",
    "            # [TODO 2.11] Define your own stopping condition here \n",
    "            if is_stop_training(all_val_loss):\n",
    "                break\n",
    "\n",
    "            if (e % epochs_to_draw == epochs_to_draw-1):\n",
    "                clear_output(wait=True)\n",
    "                plot_loss(all_train_loss, all_val_loss)\n",
    "                w_  = sess.run(w)\n",
    "                draw_weight(w_)\n",
    "                plt.show()\n",
    "                plt.pause(0.1)     \n",
    "                print(\"Epoch %d: train loss: %.5f || val loss: %.5f\" % (e+1, train_loss, val_loss))\n",
    "\n",
    "            toc = time.time()\n",
    "            print(toc-tic)\n",
    "\n",
    "\n",
    "        y_hat = sess.run(pred, feed_dict={x: test_x})\n",
    "        np.set_printoptions(precision=2)\n",
    "        confusion_mat = test(y_hat, test_y)\n",
    "        print('Confusion matrix:')\n",
    "        print(confusion_mat)\n",
    "        print('Diagonal values:')\n",
    "        print(confusion_mat.flatten()[0::11])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
